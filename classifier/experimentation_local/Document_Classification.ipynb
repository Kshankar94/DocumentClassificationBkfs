{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImport the dataset\\n\\nPerform NLP preprocessing\\n\\nApply ML models\\n\\nTraining/Testing/Hyperparameter tuning\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Import the dataset\n",
    "\n",
    "Perform NLP preprocessing\n",
    "\n",
    "Apply ML models\n",
    "\n",
    "Training/Testing/Hyperparameter tuning\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 61709\n",
       "unique                                                59753\n",
       "top       bf064c332aa1 079935e500e5 1a4dd36c6de0 7efa289...\n",
       "freq                                                     11\n",
       "Name: document_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/input_train.csv')\n",
    "\n",
    "#add column title\n",
    "#add tsne later\n",
    "df.columns = ['document_label', 'document_text']\n",
    "df = df[pd.notnull(df['document_text'])]\n",
    "\n",
    "df.shape\n",
    "X = df['document_text']\n",
    "y = df['document_label']\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61709, 1000)\n"
     ]
    }
   ],
   "source": [
    "#research more on this. \n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), min_df = 1, max_df = 0.9, max_features = 5000)\n",
    "\n",
    "# vector_content = vectorizer.fit_transform(X.apply(lambda x: np.str_(x)))\n",
    "\n",
    "# print(vector_content.shape)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "vector_content = vectorizer.fit_transform(X.apply(lambda x: np.str_(x)))\n",
    "print(vector_content.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vector_content, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle null document_text columns\n",
    "# count_nan = len(df) - df.count()\n",
    "#print(df['document_text'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import model\n",
    "svm_clf = SVC(gamma=\"scale\")\n",
    "svm_clf.fit(X_train[:30000], y_train[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 2)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('dataset/input_test_.csv')\n",
    "\n",
    "print(df_test.shape)\n",
    "#add column title\n",
    "#add tsne later\n",
    "df_test.columns = ['document_label', 'document_text']\n",
    "df_test = df_test[pd.notnull(df_test['document_text'])]\n",
    "\n",
    "df.shape\n",
    "X_t = df_test['document_text']\n",
    "y_t = df_test['document_label']\n",
    "\n",
    "df_test.head()\n",
    "\n",
    "df_test.to_csv(r'dataset/input_test_label.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172,)\n",
      "(172, 1000)\n",
      "(0.12105263157894737, 0.13355973528387322, 0.08289246693502014, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_label = pd.read_csv('dataset/input_test_label.csv')\n",
    "X_ = df_label['document_text']\n",
    "\n",
    "print(X_.shape)\n",
    "vectorize = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "vec_content = vectorize.fit_transform(X_.apply(lambda x: np.str_(x)))\n",
    "print(vec_content.shape)\n",
    "                                      \n",
    "y_t = df_label['document_label'].values.tolist()                                        \n",
    "y_label = svm_clf.predict(vec_content)   \n",
    "\n",
    "print(precision_recall_fscore_support(y_t, y_label, average='macro'))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tCLASSIFICATIION METRICS\n",
      "\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         RETURNED CHECK       0.81      0.29      0.43        45\n",
      "                   BILL       0.90      0.89      0.90      3706\n",
      "          POLICY CHANGE       0.40      0.04      0.07        55\n",
      "    CANCELLATION NOTICE       0.84      0.86      0.85      1780\n",
      "            DECLARATION       0.80      0.87      0.83      1929\n",
      "     CHANGE ENDORSEMENT       0.74      0.79      0.77       178\n",
      "     NON-RENEWAL NOTICE       0.57      0.18      0.28       193\n",
      "                 BINDER       0.90      0.89      0.89       972\n",
      "   REINSTATEMENT NOTICE       0.94      0.59      0.72       150\n",
      "   DELETION OF INTEREST       1.00      0.29      0.45        41\n",
      "      EXPIRATION NOTICE       0.94      0.70      0.80       113\n",
      "INTENT TO CANCEL NOTICE       0.78      0.86      0.82      2119\n",
      "            APPLICATION       0.92      0.91      0.92       896\n",
      "            BILL BINDER       0.98      0.75      0.85       165\n",
      "\n",
      "              micro avg       0.85      0.85      0.85     12342\n",
      "              macro avg       0.82      0.64      0.68     12342\n",
      "           weighted avg       0.85      0.85      0.85     12342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_clf.predict(X_val)\n",
    "\n",
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_val, y_pred, \n",
    "                                    target_names= df['document_label'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8232001016104897, 0.6355821553463901, 0.6830182780049482, None)\n",
      "(0.8224502012196832, 0.6332329502983584, 0.67518174611608, None)\n"
     ]
    }
   ],
   "source": [
    "print(precision_recall_fscore_support(y_val, y_pred, average='macro'))\n",
    "y_pred_test = svm_clf.predict(X_test)\n",
    "print(precision_recall_fscore_support(y_test, y_pred_test, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb validation\n",
      "(0.48614410981510753, 0.7006008662940548, 0.5179960498635788, None)\n"
     ]
    }
   ],
   "source": [
    "#multinomialNB\n",
    "NB_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = NB_classifier.predict(X_val)\n",
    "y_pred_test_NB = NB_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"nb validation\")\n",
    "print(precision_recall_fscore_support(y_val, y_pred_val, average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd validation\n",
      "(0.7633549603927785, 0.6865677312503299, 0.716039894788853, None)\n",
      "sgd test\n",
      "(0.760640696200532, 0.6814352232199178, 0.7093594917140947, None)\n",
      "(0.08339108339108338, 0.07083284956848175, 0.04370198371746359, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#sgd classifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val_ = sgd_model.predict(X_val)\n",
    "y_pred_test_sgd = sgd_model.predict(X_test)\n",
    "\n",
    "print(\"sgd validation\")\n",
    "print(precision_recall_fscore_support(y_val, y_pred_val_, average='macro'))\n",
    "\n",
    "print(\"sgd test\")\n",
    "print(precision_recall_fscore_support(y_test, y_pred_test_sgd, average='macro'))\n",
    "\n",
    "                                      \n",
    "y_lab = sgd_model.predict(vec_content)\n",
    "print(precision_recall_fscore_support(y_t, y_lab, average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 119    8   12    0    0    0    0    2    0    0    0    0    0    0]\n",
      " [   1 3199  176  214   25    2    1   49    6    4   14    1    0   13]\n",
      " [   8   80 1836   45   10   20    4   55   14   20    3    1    4    6]\n",
      " [   1   66   55 1725    5    1    2    5   34   70    4    2    0    0]\n",
      " [   2   27   50    7   46    2    0   45    0   10    1    0    1    0]\n",
      " [   0    1   34    3    1  136    0    0    0    1    1    0    0    0]\n",
      " [   0    0    6   13    0    1  126    0    0    5    0    1    0    0]\n",
      " [   2   42  215   14   46    5    1 1468    2    5    1    0    2    0]\n",
      " [   2   11   22   33    2    0    0    0  792   10    2    0    0    0]\n",
      " [   0    1    9   83    0    3    1    2    3  830    0    0    0    0]\n",
      " [   0   26    3    6    1    1    1    1    0    0  101    0    0    0]\n",
      " [   0    6    1   18    0    0    0    0    1    3    0   16    0    0]\n",
      " [   0    2   12    0    2    0    0    9    0    0    0    0   20    0]\n",
      " [   0   28   16    2    1    1    0    5    0    1    0    0    0    7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "ax= plt.subplot()\n",
    "\n",
    "labels = df[\"document_label\"].unique()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test_sgd, labels)\n",
    "print(cm)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Python27\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0)\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, vector_content, y, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Standard deviation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.851108</td>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.003330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.681181</td>\n",
       "      <td>0.002016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean Accuracy  Standard deviation\n",
       "model_name                                           \n",
       "LinearSVC                0.851108            0.001258\n",
       "LogisticRegression       0.860329            0.003330\n",
       "MultinomialNB            0.681181            0.002016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
